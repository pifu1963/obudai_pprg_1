{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical inverse problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $Ax = b$, b is known (data) and we are looking for $A^{-1}$ to get the original $x$.\n",
    "2. During the task of reconstruction we are given $b$ (the projections) and we are looking for methods to get some form of method to approximate $A^{-1}$\n",
    "3. There are many problems with $A^{-1}$, it is impossible to model explicitly and the problem (1) is ill-positioned in the Hadamard sense. So we are looking for methods, which approximately/closely solve the problem of (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Reconstruction methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most known method is the filtered-backprojection (FBP), which is based on the inverse Radon-transform as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Filtered back-projection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The forward transformation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import radon, rescale\n",
    "\n",
    "image = shepp_logan_phantom()\n",
    "image = rescale(image, scale=0.4, mode='reflect', channel_axis=None)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "\n",
    "ax1.set_title(\"Original\")\n",
    "ax1.imshow(image, cmap=plt.cm.Greys_r)\n",
    "\n",
    "theta = np.linspace(0.0, 180.0, max(image.shape), endpoint=False)\n",
    "sinogram = radon(image, theta=theta)\n",
    "dx, dy = 0.5 * 180.0 / max(image.shape), 0.5 / sinogram.shape[0]\n",
    "ax2.set_title(\"Radon transform\\n(Sinogram)\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "ax2.imshow(\n",
    "    sinogram,\n",
    "    cmap=plt.cm.Greys_r,\n",
    "    extent=(-dx, 180.0 + dx, -dy, sinogram.shape[0] + dy),\n",
    "    aspect='auto',\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The backward transformation and filters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform.radon_transform import _get_fourier_filter\n",
    "\n",
    "filters = ['ramp', 'shepp-logan', 'cosine', 'hamming', 'hann']\n",
    "\n",
    "for ix, f in enumerate(filters):\n",
    "    response = _get_fourier_filter(2000, f)\n",
    "    plt.plot(response, label=f)\n",
    "\n",
    "plt.xlim([0, 1000])\n",
    "plt.xlabel('frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from skimage.transform import iradon\n",
    "\n",
    "reconstruction_fbp = iradon(sinogram, theta=theta, filter_name='ramp')\n",
    "error = reconstruction_fbp - image\n",
    "print(f'FBP rms reconstruction error: {np.sqrt(np.mean(error**2)):.3g}')\n",
    "\n",
    "imkwargs = dict(vmin=-0.2, vmax=0.2)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5), sharex=True, sharey=True)\n",
    "ax1.set_title(\"Reconstruction\\nFiltered back projection\")\n",
    "ax1.imshow(reconstruction_fbp, cmap=plt.cm.Greys_r)\n",
    "ax2.set_title(\"Reconstruction error\\nFiltered back projection\")\n",
    "ax2.imshow(reconstruction_fbp - image, cmap=plt.cm.Greys_r, **imkwargs)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with FBP are the following:\n",
    "* Results are statistically not correct for SPECT data, which is $\\lambda$-Poisson\n",
    "* Can not be used with attenuation maps (CT) for attenuation correction\n",
    "* Good for mathematically correct estimations for calibration measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "1. Specialize it for SPECT data and do the following tasks:\n",
    "2. Load the SPECT projection data and plot them as they are in the dicom file\n",
    "3. Reconstruct the the volume (original distribution) from the projection frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Maximum-likelihood Expectation Maximization (MLEM)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for the Shepp-Logan phantom, Radon-transform, resize...\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import radon, rescale, resize\n",
    "\n",
    "import math\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# creation of MLEM datastructures and steps to run the reconstruction\n",
    "image_f = shepp_logan_phantom()\n",
    "image_f = resize(image_f, (3,3) )\n",
    "\n",
    "image_f *= 255.0/image_f.max()\n",
    "\n",
    "#print('image_f min, max: ', np.min(image_f), np.max(image_f))\n",
    "\n",
    "image_g = radon(image_f)\n",
    "\n",
    "#print('image_g min, max: ', np.min(image_g), np.max(image_g))\n",
    "\n",
    "# plt.imshow(image_g)\n",
    "# plt.show()\n",
    "\n",
    "g = image_g.copy()\n",
    "f = image_f.copy()\n",
    "\n",
    "g_row, g_col = g.shape[:]\n",
    "f_row, f_col = f.shape[:]\n",
    "\n",
    "#print('g shape: ', g_row, g_col)\n",
    "#print('f shape: ', f_row, f_col)\n",
    "#print('')\n",
    "\n",
    "#1.feladat. Az A mátrix meghatározsa\n",
    "\n",
    "gs = np.reshape(g,(g.shape[0]*g.shape[1], 1))\n",
    "fs = np.reshape(f,(f.shape[0]*f.shape[1], 1))\n",
    "\n",
    "#print('gs shape: ', gs.shape)\n",
    "#print('fs shape: ', fs.shape)\n",
    "#print('')\n",
    "\n",
    "A = np.zeros((gs.shape[0], fs.shape[0]),dtype=float)\n",
    "\n",
    "N, angles_num = g.shape[:]\n",
    "\n",
    "angles = np.linspace(0, math.pi, angles_num) # it is recommended\n",
    "\n",
    "COS = np.zeros((N,len(angles)))\n",
    "SIN = np.zeros((N,len(angles)))\n",
    "\n",
    "x = np.linspace(-0.5,0.5,N)\n",
    "\n",
    "for k in range(0,N):\n",
    "    for a in range(0,len(angles)):\n",
    "        COS[k,a] = x[k]*math.cos(angles[a])\n",
    "        SIN[k,a] = x[k]*math.sin(angles[a])\n",
    "\n",
    "f_num = 0\n",
    "\n",
    "for i in range(0, N):\n",
    "    for j in range(0, N):\n",
    "        for angle in range(0,len(angles)):\n",
    "            r = COS[i,angle] + SIN[j,angle]\n",
    "            index = (r + 0.5)*N\n",
    "            if index >= 0 and index < N:\n",
    "                ind = math.floor(index)\n",
    "                if  (index - math.floor(index)) > 0.5:\n",
    "                    A[ind    , f_num] = 1 - (index - math.floor(index))\n",
    "                    A[ind + 1, f_num] = (index - math.floor(index))\n",
    "                else:\n",
    "                    index = math.floor(index)\n",
    "                    A[ind, f_num] = (index - math.floor(index))\n",
    "                    A[ind + 1, f_num] = 1 - (index - math.floor(index))\n",
    "        f_num = f_num + 1\n",
    "\n",
    "#print('A shape: ', A.shape)\n",
    "# print('')\n",
    "#\n",
    "f0 = fs.copy()\n",
    "f1 = f0.copy()\n",
    "\n",
    "for i in range(0, f0.shape[0]):\n",
    "    if f0[i] < 0:\n",
    "        f0[i] = 0\n",
    "\n",
    "#number of pixels\n",
    "m = fs.shape[0]\n",
    "\n",
    "#number of bins\n",
    "n = gs.shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# running the actual reconstruction\n",
    "start = time.time()\n",
    "\n",
    "for k in range(0,10):\n",
    "    for j in range(0, fs.shape[0]):\n",
    "        Sa = 0\n",
    "        for i in range(0, n):\n",
    "            Sa = Sa + A[i, j]\n",
    "\n",
    "        S = 0\n",
    "        for i in range(0, n):\n",
    "            Saf = 0\n",
    "            for j_v in range(0, m):\n",
    "                Saf = Saf + A[i,j_v] * f0[j_v]\n",
    "            if Saf != 0:\n",
    "                S = S + gs[i] * A[i, j] / Saf\n",
    "        if Sa != 0:\n",
    "            f1[j] = S * f0[j] / Sa\n",
    "        #print('Iteration (j):', j)\n",
    "    f0 = f1\n",
    "    print('+++++++++++++++++++++++++++++')\n",
    "    print('Iteration: ', k)\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed time: ',(end - start)/60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ff = np.reshape(f0,(f.shape[0], f.shape[1]))\n",
    "#print(np.max(f), np.min(f))\n",
    "\n",
    "ff *= 255.0/ff.max()\n",
    "\n",
    "print('ff min, max: ', np.min(ff), np.max(ff))\n",
    "\n",
    "print(f)\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++')\n",
    "print(ff)\n",
    "#print(image_f)\n",
    "\n",
    "ax1.imshow(int(f), cmap='gray')\n",
    "ax2.imshow(int(ff), cmap='gray')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "1. Try to reconstruct the volume from the SPECT projections\n",
    "2. Run as many iterations as possible\n",
    "3. Try to parallelize the MLEM reconstruction on multiple threads, processes, interpreters ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Oredered Subsets Expectation Maximization (OSEM)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neural computation\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Perceptrons"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### A. Single-layer perceptron"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perceptron architecture with pytorch"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, output_size):\n",
    "        super(SingleLayerNet, self).__init__()\n",
    "        # Define the hidden layer with input_size input features and hidden_neurons neurons\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_neurons)\n",
    "\n",
    "        # Define the output layer with hidden_neurons input features and output_size neurons\n",
    "        self.output_layer = nn.Linear(hidden_neurons, output_size)\n",
    "\n",
    "#Define a Prediction Function\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the hidden layer and apply the sigmoid activation function\n",
    "        hidden_output = torch.sigmoid(self.hidden_layer(x))\n",
    "\n",
    "        # Pass the hidden layer output through the output layer and apply the sigmoid activation function\n",
    "        y_pred = torch.sigmoid(self.output_layer(hidden_output))\n",
    "\n",
    "        return y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a model with one neuron\n",
    "model = SingleLayerNet(1, 1, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training method of the neural network"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the loss function (criterion)\n",
    "def criterion(y_pred, y_true):\n",
    "    # Binary Cross Entropy Loss\n",
    "    # y_pred: predicted probabilities, y_true: true labels (0 or 1)\n",
    "\n",
    "    # Compute the negative log likelihood loss using binary cross-entropy formula\n",
    "    # (y * log(y_pred) + (1 - y) * log(1 - y_pred))\n",
    "    loss = -1 * (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "    # Calculate the mean loss over the batch\n",
    "    mean_loss = torch.mean(loss)\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "# Assuming 'model' is an instance of your custom neural network\n",
    "# Create an optimizer (Stochastic Gradient Descent - SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generation of the training data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate synthetic data for X ranging from -30 to 29 with a step size of 1\n",
    "X = torch.arange(-30, 30, 1).view(-1, 1).type(torch.FloatTensor)\n",
    "\n",
    "# Initialize an empty tensor Y to store the labels (target values)\n",
    "Y = torch.zeros(X.shape[0])\n",
    "\n",
    "# Assign label 1.0 to elements in Y where the corresponding X value is less than or equal to -10\n",
    "Y[X[:, 0] <= -10] = 1.0\n",
    "\n",
    "# Assign label 0.5 to elements in Y where the corresponding X value falls between -10 and 10 (exclusive)\n",
    "Y[(X[:, 0] > -10) & (X[:, 0] < 10)] = 0.5\n",
    "\n",
    "# Assign label 0 to elements in Y where the corresponding X value is greater than 10\n",
    "Y[X[:, 0] > 10] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the actual training loop"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the training loop\n",
    "epochs = 5000\n",
    "cost = []  # List to store the total loss at each epoch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0  # Variable to store the total loss for the current epoch\n",
    "    epoch = epoch + 1  # Increment the epoch count\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        # Forward pass: Calculate the predicted output (yhat) using the model\n",
    "        yhat = model(x)\n",
    "\n",
    "        # Calculate the loss between the predicted output (yhat) and the actual target (y)\n",
    "        loss = criterion(yhat, y)\n",
    "\n",
    "        # Backpropagation: Compute gradients of the model parameters with respect to the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero out the gradients for the next iteration to avoid accumulation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate the loss for this batch of data\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Append the total loss for this epoch to the cost list\n",
    "    cost.append(total_loss)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch} done!\")  # Print status after every 1000 epochs\n",
    "\n",
    "        # Plot the result of the function approximator\n",
    "        predicted_values = model(X).detach().numpy()\n",
    "        plt.plot(X.numpy(), predicted_values)  # Plot the predicted values\n",
    "        plt.plot(X.numpy(), Y.numpy(), 'm')  # Plot the ground truth data (Y)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend(['Predicted', 'Ground Truth'])\n",
    "        plt.title(f'Epoch {epoch} - Function Approximation')\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plotting the actual loss function during training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the cost (loss) over epochs\n",
    "plt.plot(cost, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Training Progress - Cross Entropy Loss')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### B. Multi-layer perceptron"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Multi-layer perceptron architecture in Pytroch with forward and backward pass"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "        self.b1 = torch.randn(1, hidden_size, requires_grad=True)\n",
    "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
    "        self.b2 = torch.randn(1, output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = torch.matmul(X, self.W1) + self.b1\n",
    "        self.a1 = torch.sigmoid(self.z1)  # Hidden layer activation\n",
    "        self.z2 = torch.matmul(self.a1, self.W2) + self.b2\n",
    "        self.a2 = torch.sigmoid(self.z2)  # Output layer activation\n",
    "        return self.a2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, lr=0.01):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            #Compute loss using (Mean Squared Error)\n",
    "            loss = torch.mean((output - y) ** 2)\n",
    "            losses.append(loss.item())\n",
    "            #update weights\n",
    "            self.backward(X, y, output, lr)\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        return losses\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initializing the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_size = X.size()[0]\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "model = SimpleMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "#Train  model and store the losses\n",
    "losses = model.train(X, Y, epochs=1000, lr=0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plotting the loss function"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Generation of test data",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    test_output = model.forward(X)\n",
    "    test_output = (test_output > 0.5).float()\n",
    "accuracy = torch.mean((test_output == Y).float())\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Task 1. Solve Filtered Backprojection with SLPs"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Task 2. Solve Filtered Backprojection with MLPs"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reconstruction with Autoencoders"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
